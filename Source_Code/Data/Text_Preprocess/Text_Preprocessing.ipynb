{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\quang binh\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\quang binh\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\quang binh\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "Requirement already satisfied: anyascii in c:\\users\\quang binh\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.0 is available.\n",
      "You should consider upgrading via the 'c:\\users\\quang binh\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(2020)\n",
    "\n",
    "import nltk\n",
    "nltk.download('all') # one time execution\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'/content/drive/MyDrive/data_final.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna()\n",
    "data = data.drop(columns=['Unnamed: 0'])\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns = {\"song name\": \"song_name\",\n",
    "                              \"song lyric\": \"song_lyric\"})\n",
    "data = data.astype(np.str)\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import contractions\n",
    "contractions_dict = {\"ain't\": \"are not\",\n",
    "                     \"'s\":\" is\",\n",
    "                     \"aren't\": \"are not\",\n",
    "                     \"'fore\": \"before\", \n",
    "                     \"i've\": \"i have\", \n",
    "                     \"you've\": \"you have\",\n",
    "                     \"wanna\": \"want to\", \n",
    "                     \"gotta\": \"have got to\",\n",
    "                     \"gonna\": \"going to\",\n",
    "                     \"ima\": \"i am going to\",\n",
    "                     \"you'll\": \"you will\",\n",
    "                     \"i'll\": \"i will\"}\n",
    "\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "\n",
    "    def __init__(self, dataframe):    \n",
    "        self.df = dataframe\n",
    "\n",
    "    # delete punctuations\n",
    "    def remove_punctuation(self, text):       \n",
    "        return re.sub(f'[{string.punctuation}]', '', text)\n",
    "\n",
    "    # lower texts\n",
    "    def to_lowercase(self, text):       \n",
    "        return text.lower()\n",
    "    \n",
    "     # remove numbers\n",
    "    def remove_numbers(self, text):     \n",
    "        return re.sub(r'\\d+', '', text)\n",
    "    \n",
    "     # remove extra white spaces\n",
    "    def remove_whitespaces(self, text):      \n",
    "        return text.strip()\n",
    "    \n",
    "     # expand contradiction\n",
    "    def expand_contraction(self, text, contractions_dict = contractions_dict):     \n",
    "        expanded_words = []   \n",
    "        for word in text.split():\n",
    "            expanded_words.append(contractions.fix(word))  \n",
    "        expanded_text = ' '.join(expanded_words)\n",
    "\n",
    "        def replace(match):\n",
    "            return contractions_dict[match.group(0)]\n",
    "        return contractions_re.sub(replace, expanded_text)\n",
    "    \n",
    "     # delete non-english words\n",
    "    def del_nonEnglish(self, text):      \n",
    "        text = re.sub(r'\\W+', ' ', text)\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"[^a-zA-Z]\", \" \")\n",
    "        word_tokens = word_tokenize(text)\n",
    "        filtered_word = [w for w in word_tokens if all(ord(c) < 128 for c in w)]\n",
    "        filtered_word = [w + \" \" for w in filtered_word]\n",
    "        return \"\".join(filtered_word)\n",
    "    \n",
    "     # remove stopwords\n",
    "    def remove_stopwords(self, text):    \n",
    "        global stop_words\n",
    "        try:\n",
    "            word_tokens = word_tokenize(text)\n",
    "            filtered_word = [w for w in word_tokens if not w in stop_words]\n",
    "            filtered_word = [w + \" \" for w in filtered_word]\n",
    "            return \"\".join(filtered_word)\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "     #lemmatize\n",
    "    def normalization(self, text):      \n",
    "        global lemmatizer\n",
    "        return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    \n",
    "    def preprocess_text(self):\n",
    "        self.df['name_pp'] = self.df.song_name.apply(self.to_lowercase)\n",
    "        self.df['name_pp'] = self.df.name_pp.apply(self.expand_contraction)\n",
    "        self.df['name_pp'] = self.df.name_pp.apply(self.remove_punctuation)\n",
    "        self.df['name_pp'] = self.df.name_pp.apply(self.del_nonEnglish)\n",
    "        self.df['name_pp'] = self.df.name_pp.apply(self.remove_stopwords)\n",
    "        self.df['name_pp'] = self.df.name_pp.apply(self.normalization)\n",
    "        self.df['name_pp'] = self.df.name_pp.apply(self.remove_whitespaces)\n",
    "        self.df['name_pp'] = self.df.name_pp.apply(self.remove_numbers)\n",
    "\n",
    "        self.df['lyric_pp'] = self.df.song_lyric.apply(self.to_lowercase)\n",
    "        self.df['lyric_pp'] = self.df.lyric_pp.apply(self.expand_contraction)\n",
    "        self.df['lyric_pp'] = self.df.lyric_pp.apply(self.remove_punctuation)\n",
    "        self.df['lyric_pp'] = self.df.lyric_pp.apply(self.del_nonEnglish)\n",
    "        self.df['lyric_pp'] = self.df.lyric_pp.apply(self.remove_stopwords)\n",
    "        self.df['lyric_pp'] = self.df.lyric_pp.apply(self.normalization)\n",
    "        self.df['lyric_pp'] = self.df.lyric_pp.apply(self.remove_whitespaces)\n",
    "        self.df['lyric_pp'] = self.df.lyric_pp.apply(self.remove_numbers)\n",
    "        \n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextPreprocessor(data).preprocess_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['song_name', 'song_lyric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'/content/drive/MyDrive/data_final_preprocessed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ca2561c17da07f33809ca03ba7aa600fbdf40d422ce225f1db7d8c88545dfa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
